{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# if you like it ,please vote for it,good score for you ( •̀ ω •́ )✧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V1: seresnet101 0.959\n",
    "\n",
    "V6: B7+InceprtionresnetV2 0.974\n",
    "\n",
    "V7: image_size=800 0.974\n",
    "\n",
    "V8:replace optimizer 0.978\n",
    "\n",
    "V9:weight=noisy-student,\n",
    "\n",
    "V10:5 K-fold+SoftProbField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import efficientnet.tfkeras as efn\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPU Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Create strategy from tpu\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "# Data access\n",
    "GCS_DS_PATH = KaggleDatasets().get_gcs_path()\n",
    "\n",
    "# Configuration\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_path(st):\n",
    "    return GCS_DS_PATH + '/images/' + st + '.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/plant-pathology-2020-fgvc7/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/plant-pathology-2020-fgvc7/test.csv')\n",
    "sub = pd.read_csv('/kaggle/input/plant-pathology-2020-fgvc7/sample_submission.csv')\n",
    "\n",
    "train_paths = train.image_id.apply(format_path).values\n",
    "test_paths = test.image_id.apply(format_path).values\n",
    "\n",
    "train_labels = train.loc[:, 'healthy':].values\n",
    "\n",
    "## если планируете обучать модель с валидирующим набором данных\n",
    "# train_paths, valid_paths, train_labels, valid_labels = train_test_split(\n",
    "#     train_paths, train_labels, test_size=0.15, random_state=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 850"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(filename, label=None, image_size=(image_size, image_size)):\n",
    "    bits = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(bits, channels=3)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.image.resize(image, image_size)\n",
    "    \n",
    "    if label is None:\n",
    "        return image\n",
    "    else:\n",
    "        return image, label\n",
    "\n",
    "def data_augment(image, label=None):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    \n",
    "    if label is None:\n",
    "        return image\n",
    "    else:\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "tf.data.Dataset\n",
    "    .from_tensor_slices((train_paths, train_labels))\n",
    "    .map(decode_image, num_parallel_calls=AUTO)\n",
    "    .cache()\n",
    "    .map(data_augment, num_parallel_calls=AUTO)\n",
    "    .repeat()\n",
    "    .shuffle(512)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "train_dataset_1 = (\n",
    "tf.data.Dataset\n",
    "    .from_tensor_slices((train_paths, train_labels))\n",
    "    .map(decode_image, num_parallel_calls=AUTO)\n",
    "    .cache()\n",
    "    .map(data_augment, num_parallel_calls=AUTO)\n",
    "    .repeat()\n",
    "    .shuffle(512)\n",
    "    .batch(64)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "# valid_dataset = (\n",
    "#     tf.data.Dataset\n",
    "#     .from_tensor_slices((valid_paths, valid_labels))\n",
    "#     .map(decode_image, num_parallel_calls=AUTO)\n",
    "#     .batch(BATCH_SIZE)\n",
    "#     .cache()\n",
    "#     .prefetch(AUTO)\n",
    "# )\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(test_paths)\n",
    "    .map(decode_image, num_parallel_calls=AUTO)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate schedule: 0.0001 to 0.0004 to 0.0001\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "LR_START = 0.0001\n",
    "LR_MAX = 0.00005 * strategy.num_replicas_in_sync\n",
    "LR_MIN = 0.0001\n",
    "LR_RAMPUP_EPOCHS = 4\n",
    "LR_SUSTAIN_EPOCHS = 6\n",
    "LR_EXP_DECAY = .8\n",
    "\n",
    "def lrfn(epoch):\n",
    "    if epoch < LR_RAMPUP_EPOCHS:\n",
    "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
    "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
    "        lr = LR_MAX\n",
    "    else:\n",
    "        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n",
    "    return lr\n",
    "    \n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n",
    "\n",
    "rng = [i for i in range(EPOCHS)]\n",
    "y = [lrfn(x) for x in rng]\n",
    "plt.plot(rng, y)\n",
    "print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class SoftProbField(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SoftProbField, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(SoftProbField, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        h = x[:, 0]\n",
    "        s = x[:, 1]\n",
    "        r = x[:, 2]\n",
    "        \n",
    "        m = s*r*(1-h)\n",
    "        s = s*(1-h)*(1-m)\n",
    "        r = r*(1-h)*(1-m)\n",
    "        return tf.stack([h, m, r, s], axis=-1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.7/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "219062272/219055592 [==============================] - 7s 0us/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_resnet_v2 (Model)  (None, 25, 25, 1536)      54336736  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 4611      \n",
      "_________________________________________________________________\n",
      "soft_prob_field (SoftProbFie (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 54,341,347\n",
      "Trainable params: 54,280,803\n",
      "Non-trainable params: 60,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential([\n",
    "        InceptionResNetV2(\n",
    "            input_shape=(image_size, image_size, 3),\n",
    "            weights='imagenet',\n",
    "            include_top=False\n",
    "        ),\n",
    "        L.GlobalAveragePooling2D(),\n",
    "        L.Dense(3, activation='sigmoid'),\n",
    "        SoftProbField()\n",
    "    ])\n",
    "        \n",
    "    model.compile(\n",
    "        optimizer = 'adam',\n",
    "        loss = 'categorical_crossentropy',\n",
    "        metrics=['categorical_accuracy']\n",
    "    )\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 14 steps\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 1/40\n",
      "14/14 [==============================] - 306s 22s/step - loss: 1.0425 - categorical_accuracy: 0.5435\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00017500000000000003.\n",
      "Epoch 2/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.3461 - categorical_accuracy: 0.8906\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.00025.\n",
      "Epoch 3/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.1921 - categorical_accuracy: 0.9436\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00032500000000000004.\n",
      "Epoch 4/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.1457 - categorical_accuracy: 0.9570\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 5/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.1160 - categorical_accuracy: 0.9676\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 6/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.1031 - categorical_accuracy: 0.9721\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 7/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.1046 - categorical_accuracy: 0.9693\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 8/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.1176 - categorical_accuracy: 0.9671\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 9/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0681 - categorical_accuracy: 0.9821\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 10/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0648 - categorical_accuracy: 0.9821\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 11/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0599 - categorical_accuracy: 0.9849\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.00034.\n",
      "Epoch 12/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0335 - categorical_accuracy: 0.9900\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00029200000000000005.\n",
      "Epoch 13/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0494 - categorical_accuracy: 0.9905\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.00025360000000000004.\n",
      "Epoch 14/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0449 - categorical_accuracy: 0.9855\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.00022288000000000006.\n",
      "Epoch 15/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0186 - categorical_accuracy: 0.9972\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00019830400000000006.\n",
      "Epoch 16/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0122 - categorical_accuracy: 0.9983\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.00017864320000000004.\n",
      "Epoch 17/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0107 - categorical_accuracy: 0.9978\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00016291456000000005.\n",
      "Epoch 18/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0077 - categorical_accuracy: 0.9983\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00015033164800000003.\n",
      "Epoch 19/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0052 - categorical_accuracy: 0.9994\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00014026531840000004.\n",
      "Epoch 20/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0120 - categorical_accuracy: 0.9961\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.00013221225472000002.\n",
      "Epoch 21/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0071 - categorical_accuracy: 0.9978\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.00012576980377600002.\n",
      "Epoch 22/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0048 - categorical_accuracy: 0.9989\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.00012061584302080001.\n",
      "Epoch 23/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0031 - categorical_accuracy: 0.9994\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.00011649267441664002.\n",
      "Epoch 24/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0044 - categorical_accuracy: 0.9989\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.00011319413953331202.\n",
      "Epoch 25/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0016 - categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.00011055531162664962.\n",
      "Epoch 26/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0022 - categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0001084442493013197.\n",
      "Epoch 27/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0031 - categorical_accuracy: 0.9994\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.00010675539944105576.\n",
      "Epoch 28/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0037 - categorical_accuracy: 0.9989\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0001054043195528446.\n",
      "Epoch 29/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0037 - categorical_accuracy: 0.9994\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.00010432345564227568.\n",
      "Epoch 30/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0046 - categorical_accuracy: 0.9983\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.00010345876451382055.\n",
      "Epoch 31/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0040 - categorical_accuracy: 0.9989\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.00010276701161105644.\n",
      "Epoch 32/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0092 - categorical_accuracy: 0.9983\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.00010221360928884516.\n",
      "Epoch 33/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0051 - categorical_accuracy: 0.9983\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.00010177088743107613.\n",
      "Epoch 34/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0027 - categorical_accuracy: 0.9994\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.0001014167099448609.\n",
      "Epoch 35/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0033 - categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.00010113336795588872.\n",
      "Epoch 36/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0013 - categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.00010090669436471098.\n",
      "Epoch 37/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0028 - categorical_accuracy: 0.9989\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.00010072535549176879.\n",
      "Epoch 38/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0042 - categorical_accuracy: 0.9989\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.00010058028439341503.\n",
      "Epoch 39/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0022 - categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.00010046422751473202.\n",
      "Epoch 40/40\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.0056 - categorical_accuracy: 0.9978\n"
     ]
    }
   ],
   "source": [
    "\n",
    "STEPS_PER_EPOCH = train_labels.shape[0] // BATCH_SIZE\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset, \n",
    "    epochs=EPOCHS, \n",
    "    callbacks=[lr_callback],\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    #validation_data=valid_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b7_noisy-student_notop.h5\n",
      "258072576/258068648 [==============================] - 8s 0us/step\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnet-b7 (Model)      (None, 27, 27, 2560)      64097680  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 7683      \n",
      "_________________________________________________________________\n",
      "soft_prob_field_1 (SoftProbF (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 64,105,363\n",
      "Trainable params: 63,794,643\n",
      "Non-trainable params: 310,720\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model2 = tf.keras.Sequential([\n",
    "        efn.EfficientNetB7(\n",
    "            input_shape=(image_size, image_size, 3),\n",
    "            weights='noisy-student',\n",
    "            include_top=False\n",
    "        ),\n",
    "        L.GlobalAveragePooling2D(),\n",
    "        L.Dense(3, activation='sigmoid'),\n",
    "        SoftProbField()\n",
    "    ])\n",
    "        \n",
    "    model2.compile(\n",
    "        optimizer = 'adam',\n",
    "        loss = 'categorical_crossentropy',\n",
    "        metrics=['categorical_accuracy']\n",
    "    )\n",
    "    model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 28 steps\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 1/40\n",
      "28/28 [==============================] - 354s 13s/step - loss: 1.1327 - categorical_accuracy: 0.4994\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00017500000000000003.\n",
      "Epoch 2/40\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.4781 - categorical_accuracy: 0.8544\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.00025.\n",
      "Epoch 3/40\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.3332 - categorical_accuracy: 0.9012\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00032500000000000004.\n",
      "Epoch 4/40\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.3190 - categorical_accuracy: 0.9063\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 5/40\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.2500 - categorical_accuracy: 0.9247\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 6/40\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.2763 - categorical_accuracy: 0.9235\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 7/40\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.2156 - categorical_accuracy: 0.9319\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 8/40\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.1715 - categorical_accuracy: 0.9559\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 9/40\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.1352 - categorical_accuracy: 0.9576\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 10/40\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.1470 - categorical_accuracy: 0.9554\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 11/40\n",
      "28/28 [==============================] - 37s 1s/step - loss: 0.1151 - categorical_accuracy: 0.9665\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.00034.\n",
      "Epoch 12/40\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.1063 - categorical_accuracy: 0.9693\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00029200000000000005.\n",
      "Epoch 13/40\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.0731 - categorical_accuracy: 0.9788\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.00025360000000000004.\n",
      "Epoch 14/40\n",
      "28/28 [==============================] - 37s 1s/step - loss: 0.0523 - categorical_accuracy: 0.9872\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.00022288000000000006.\n",
      "Epoch 15/40\n",
      "28/28 [==============================] - 37s 1s/step - loss: 0.0566 - categorical_accuracy: 0.9849\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00019830400000000006.\n",
      "Epoch 16/40\n",
      "28/28 [==============================] - 37s 1s/step - loss: 0.0367 - categorical_accuracy: 0.9916\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.00017864320000000004.\n",
      "Epoch 17/40\n",
      "28/28 [==============================] - 37s 1s/step - loss: 0.0185 - categorical_accuracy: 0.9955\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00016291456000000005.\n",
      "Epoch 18/40\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.0404 - categorical_accuracy: 0.9900\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00015033164800000003.\n",
      "Epoch 19/40\n",
      "28/28 [==============================] - 37s 1s/step - loss: 0.0263 - categorical_accuracy: 0.9955\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00014026531840000004.\n",
      "Epoch 20/40\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.0306 - categorical_accuracy: 0.9911\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.00013221225472000002.\n",
      "Epoch 21/40\n",
      "28/28 [==============================] - 37s 1s/step - loss: 0.0180 - categorical_accuracy: 0.9950\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.00012576980377600002.\n",
      "Epoch 22/40\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.0214 - categorical_accuracy: 0.9939\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.00012061584302080001.\n",
      "Epoch 23/40\n",
      "28/28 [==============================] - 36s 1s/step - loss: 0.0187 - categorical_accuracy: 0.9944\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.00011649267441664002.\n",
      "Epoch 24/40\n",
      "28/28 [==============================] - 37s 1s/step - loss: 0.0211 - categorical_accuracy: 0.9955\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.00011319413953331202.\n",
      "Epoch 25/40\n",
      "28/28 [==============================] - 37s 1s/step - loss: 0.0094 - categorical_accuracy: 0.9994\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.00011055531162664962.\n",
      "Epoch 26/40\n",
      "28/28 [==============================] - 37s 1s/step - loss: 0.0136 - categorical_accuracy: 0.9961\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0001084442493013197.\n",
      "Epoch 27/40\n",
      "28/28 [==============================] - 37s 1s/step - loss: 0.0124 - categorical_accuracy: 0.9961\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.00010675539944105576.\n",
      "Epoch 28/40\n",
      "28/28 [==============================] - 37s 1s/step - loss: 0.0107 - categorical_accuracy: 0.9989\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0001054043195528446.\n",
      "Epoch 29/40\n",
      "28/28 [==============================] - 37s 1s/step - loss: 0.0223 - categorical_accuracy: 0.9939\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.00010432345564227568.\n",
      "Epoch 30/40\n",
      "28/28 [==============================] - 37s 1s/step - loss: 0.0109 - categorical_accuracy: 0.9978\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.00010345876451382055.\n",
      "Epoch 31/40\n",
      "28/28 [==============================] - 37s 1s/step - loss: 0.0137 - categorical_accuracy: 0.9967\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.00010276701161105644.\n",
      "Epoch 32/40\n",
      "28/28 [==============================] - 37s 1s/step - loss: 0.0087 - categorical_accuracy: 0.9972\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.00010221360928884516.\n",
      "Epoch 33/40\n",
      "28/28 [==============================] - 37s 1s/step - loss: 0.0071 - categorical_accuracy: 0.9983\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.00010177088743107613.\n",
      "Epoch 34/40\n",
      "28/28 [==============================] - 37s 1s/step - loss: 0.0070 - categorical_accuracy: 0.9989\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.0001014167099448609.\n",
      "Epoch 35/40\n",
      "28/28 [==============================] - 38s 1s/step - loss: 0.0070 - categorical_accuracy: 0.9989\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.00010113336795588872.\n",
      "Epoch 36/40\n",
      "28/28 [==============================] - 37s 1s/step - loss: 0.0134 - categorical_accuracy: 0.9961\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.00010090669436471098.\n",
      "Epoch 37/40\n",
      "28/28 [==============================] - 37s 1s/step - loss: 0.0064 - categorical_accuracy: 0.9983\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.00010072535549176879.\n",
      "Epoch 38/40\n",
      "28/28 [==============================] - 37s 1s/step - loss: 0.0082 - categorical_accuracy: 0.9978\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.00010058028439341503.\n",
      "Epoch 39/40\n",
      "28/28 [==============================] - 37s 1s/step - loss: 0.0139 - categorical_accuracy: 0.9950\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.00010046422751473202.\n",
      "Epoch 40/40\n",
      "28/28 [==============================] - 37s 1s/step - loss: 0.0127 - categorical_accuracy: 0.9978\n"
     ]
    }
   ],
   "source": [
    "STEPS_PER_EPOCH = train_labels.shape[0] // 64\n",
    "\n",
    "history = model2.fit(\n",
    "    train_dataset_1, \n",
    "    epochs=EPOCHS, \n",
    "    callbacks=[lr_callback],\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    #validation_data=valid_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_training_curves(training, validation, title, subplot):\n",
    "    \"\"\"\n",
    "    Source: https://www.kaggle.com/mgornergoogle/getting-started-with-100-flowers-on-tpu\n",
    "    \"\"\"\n",
    "    if subplot%10==1: # set up the subplots on the first call\n",
    "        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n",
    "        plt.tight_layout()\n",
    "    ax = plt.subplot(subplot)\n",
    "    ax.set_facecolor('#F8F8F8')\n",
    "    ax.plot(training)\n",
    "    ax.plot(validation)\n",
    "    ax.set_title('model '+ title)\n",
    "    ax.set_ylabel(title)\n",
    "    #ax.set_ylim(0.28,1.05)\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.legend(['train', 'valid.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-62119d05cea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m display_training_curves(\n\u001b[1;32m      2\u001b[0m     \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     'loss', 211)\n\u001b[1;32m      5\u001b[0m display_training_curves(\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "display_training_curves(\n",
    "    history.history['loss'], \n",
    "    history.history['val_loss'], \n",
    "    'loss', 211)\n",
    "display_training_curves(\n",
    "    history.history['categorical_accuracy'], \n",
    "    history.history['val_categorical_accuracy'], \n",
    "    'accuracy', 212)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 109s 7s/step\n",
      "15/15 [==============================] - 75s 5s/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>healthy</th>\n",
       "      <th>multiple_diseases</th>\n",
       "      <th>rust</th>\n",
       "      <th>scab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test_0</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>9.028376e-06</td>\n",
       "      <td>0.992918</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test_1</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>2.265832e-04</td>\n",
       "      <td>0.880206</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test_2</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>7.996453e-05</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.994480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Test_3</td>\n",
       "      <td>0.999910</td>\n",
       "      <td>3.115902e-07</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Test_4</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>2.631031e-04</td>\n",
       "      <td>0.927643</td>\n",
       "      <td>0.000305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id   healthy  multiple_diseases      rust      scab\n",
       "0   Test_0  0.000012       9.028376e-06  0.992918  0.000009\n",
       "1   Test_1  0.000040       2.265832e-04  0.880206  0.000298\n",
       "2   Test_2  0.000180       7.996453e-05  0.000080  0.994480\n",
       "3   Test_3  0.999910       3.115902e-07  0.000022  0.000001\n",
       "4   Test_4  0.000344       2.631031e-04  0.927643  0.000305"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs1 = model.predict(test_dataset, verbose=1)\n",
    "probs2 = model2.predict(test_dataset, verbose=1)\n",
    "probs_avg = (probs1+probs2)/2\n",
    "sub.loc[:, 'healthy':] = probs_avg\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
